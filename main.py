"""
Korean Grammar RAG System - Main Execution Script (A100 ìµœì í™” ë²„ì „)
í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” RAG ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python main.py --mode demo                    # ë°ëª¨ ì‹¤í–‰
    python main.py --mode evaluate --samples 10  # í‰ê°€ ì‹¤í–‰
    python main.py --mode test --enable_llm      # LLM í™œì„±í™” í…ŒìŠ¤íŠ¸
    python main.py --mode predict --data_path test.json --output_path predictions.json  # ì˜ˆì¸¡ ì‹¤í–‰
"""

import argparse
import json
import time
import traceback
import torch
from pathlib import Path
from typing import Dict, List, Any, Optional

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from rag_pipeline import KoreanGrammarRAGSystem, create_rag_system, quick_test
from utils import DataLoader, MemoryManager, setup_a100_environment

def demo_mode():
    """ë°ëª¨ ëª¨ë“œ - ëª‡ ê°œ ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    print("ğŸ­ Demo Mode - Korean Grammar RAG System")
    print("=" * 60)

    # A100 í™˜ê²½ ìµœì í™”
    if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name():
        setup_a100_environment()

    rag_system = None
    try:
        # ì‹œìŠ¤í…œ ìƒì„± (LLM ë¹„í™œì„±í™”ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        print("ğŸš€ Creating RAG System (Template Mode)...")
        rag_system = create_rag_system(enable_llm=False)

        # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
        train_data_path = './/korean_language_rag_V1.0_train.json'
        if not Path(train_data_path).exists():
            print(f"âŒ Training data not found: {train_data_path}")
            return

        rag_system.load_knowledge_base(train_data_path)

        # ë°ëª¨ ì§ˆë¬¸ë“¤
        demo_questions = [
            {
                "question": "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
                "type": "ì„ íƒí˜•"
            },
            {
                "question": "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\"ì™¸ì¶œì‹œì—ëŠ” ì—ì–´ì»¨ì„ ê¼­ ë•ì‹œë‹¤.\"",
                "type": "êµì •í˜•"
            },
            {
                "question": "{ê²€/ê»Œ}ì„ ì”¹ë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
                "type": "ì„ íƒí˜•"
            }
        ]

        # ê° ì§ˆë¬¸ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)
        for i, demo in enumerate(demo_questions, 1):
            print(f"\nğŸ“ Demo Question {i}: {demo['type']}")
            print(f"Q: {demo['question']}")
            print("-" * 40)

            # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬
            MemoryManager.check_memory_status()

            start_time = time.time()
            
            # ë©”ëª¨ë¦¬ ì•ˆì „ ì²˜ë¦¬
            def process_demo():
                return rag_system.process_question(demo['question'], demo['type'])
            
            processing_time = time.time() - start_time

            try:
                result = rag_system.process_question_optimized(question_data)
            except Exception as e:
                print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                result = None
            
            if result:
                # ê²°ê³¼ ì €ì¥
                sample_result = {
                    'id': item.get('id', f'sample_{i}'),
                    'input': item['input'],
                    'predicted_answer': result.get('predicted_answer') or "ì²˜ë¦¬ ì‹¤íŒ¨",
                    'contexts_used': result.get('contexts_used', 0)
                }

            # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
            MemoryManager.clear_gpu_memory(force=True)

        print(f"\nâœ… Demo completed successfully!")

    except Exception as e:
        print(f"âŒ Demo ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        if rag_system:
            rag_system.cleanup()
        MemoryManager.clear_gpu_memory(force=True)

def evaluate_mode(max_samples=10, enable_llm=False):
    """í‰ê°€ ëª¨ë“œ - ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í‰ê°€ (A100 ìµœì í™”)"""
    print(f"ğŸ“Š Evaluation Mode (LLM: {enable_llm}, Samples: {max_samples})")
    print("=" * 60)

    # A100 í™˜ê²½ ìµœì í™”
    if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name():
        setup_a100_environment()

    rag_system = None
    try:
        # ì‹œìŠ¤í…œ ìƒì„±
        rag_system = create_rag_system(enable_llm=enable_llm)

        # ë°ì´í„° ê²½ë¡œ í™•ì¸
        train_data_path = './/korean_language_rag_V1.0_train.json'
        dev_data_path = './/korean_language_rag_V1.0_dev.json'

        if not Path(train_data_path).exists():
            print(f"âŒ Training data not found: {train_data_path}")
            return

        if not Path(dev_data_path).exists():
            print(f"âŒ Development data not found: {dev_data_path}")
            return

        # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
        rag_system.load_knowledge_base(train_data_path)

        # í‰ê°€ ì‹¤í–‰ (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „)
        output_path = f'.//evaluation_results_llm_{enable_llm}.json'

        start_time = time.time()
        metrics = run_optimized_evaluation(
            rag_system, 
            dev_data_path, 
            output_path, 
            max_samples
        )
        total_time = time.time() - start_time

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“ˆ Evaluation Results:")
        print(f"   Accuracy: {metrics['accuracy']:.4f}")
        print(f"   Correct: {metrics['correct_predictions']}/{metrics['total_samples']}")
        print(f"   Avg Contexts: {metrics['average_contexts_used']:.2f}")
        print(f"   Total Time: {total_time:.2f}s")
        print(f"   Time per Sample: {total_time/metrics['total_samples']:.2f}s")

    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        if rag_system:
            rag_system.cleanup()
        MemoryManager.clear_gpu_memory(force=True)

def run_optimized_evaluation(rag_system, data_path: str, output_path: str, max_samples: int) -> Dict:
    """A100 ìµœì í™” í‰ê°€ ì‹¤í–‰"""
    print("ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™” í‰ê°€ ì‹œì‘...")
    
    # ë°ì´í„° ë¡œë“œ
    data = DataLoader.load_json_dataset(data_path)
    if not data:
        return {"accuracy": 0, "total_samples": 0, "correct_predictions": 0, "average_contexts_used": 0}
    
    # ìƒ˜í”Œ ìˆ˜ ì œí•œ
    if max_samples > 0:
        data = data[:max_samples]
    
    results = []
    correct_count = 0
    total_contexts = 0
    
    print(f"ğŸ“‹ ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜: {len(data)}")
    
    for i, item in enumerate(data):
        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {i+1}/{len(data)} - ID: {item.get('id', 'N/A')}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ ì²´í¬ ë° ìë™ ì •ë¦¬
        MemoryManager.auto_cleanup_if_needed()
        
        try:
            # ì•ˆì „í•œ ì§ˆë¬¸ ì²˜ë¦¬
            def process_item():
                question_data = item['input']
                return rag_system.process_question_optimized(question_data)
            
            result = MemoryManager.safe_model_operation(process_item)
            
            if result:
                # ê²°ê³¼ ì €ì¥
                sample_result = {
                    'id': item.get('id', f'sample_{i}'),
                    'input': item['input'],
                    'predicted_answer': result.get('final_answer') or result.get('rankrag_answer') or "ì²˜ë¦¬ ì‹¤íŒ¨",
                    'contexts_used': len(result.get('reranked_contexts', []))
                }
                
                results.append(sample_result)
                total_contexts += sample_result['contexts_used']
                
                # ì •ë‹µ í™•ì¸ (ground truthê°€ ìˆëŠ” ê²½ìš°)
                if 'output' in item:
                    predicted = sample_result['predicted_answer']
                    ground_truth = item['output'].get('answer', '')
                    if predicted and ground_truth:
                        from utils import EvaluationMetrics
                        if EvaluationMetrics.exact_match(predicted, ground_truth):
                            correct_count += 1
                
                print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: ID {sample_result['id']}")
                
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ê²°ê³¼ ì¶”ê°€
                sample_result = {
                    'id': item.get('id', f'sample_{i}'),
                    'input': item['input'],
                    'predicted_answer': "ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì²˜ë¦¬ ì‹¤íŒ¨",
                    'contexts_used': 0
                }
                results.append(sample_result)
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: ID {sample_result['id']}")
        
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒí•´ë„ ê²°ê³¼ ì¶”ê°€
            error_result = {
                'id': item.get('id', f'sample_{i}'),
                'input': item['input'],
                'predicted_answer': f"ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}",
                'contexts_used': 0
            }
            results.append(error_result)
        
        # ì¤‘ê°„ ì €ì¥ (5ê°œë§ˆë‹¤)
        if (i + 1) % 5 == 0:
            DataLoader.save_intermediate_results(results, output_path, i+1)
        
        # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.clear_gpu_memory()
    
    # ìµœì¢… ê²°ê³¼ ê³„ì‚°
    metrics = {
        'accuracy': correct_count / len(results) if results else 0,
        'total_samples': len(results),
        'correct_predictions': correct_count,
        'average_contexts_used': total_contexts / len(results) if results else 0
    }
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    final_results = {
        'evaluation_metrics': metrics,
        'predictions': results
    }
    
    DataLoader.save_results(final_results, output_path)
    print(f"ğŸ’¾ Results saved to: {output_path}")
    
    return metrics

def predict_mode(data_path: str, output_path: str, enable_llm: bool = True):
    """ì˜ˆì¸¡ ëª¨ë“œ - í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±"""
    print(f"ğŸ”® Prediction Mode - {data_path} â†’ {output_path}")
    print("=" * 60)
    
    # A100 í™˜ê²½ ìµœì í™”
    if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name():
        setup_a100_environment()
    
    rag_system = None
    try:
        # ì…ë ¥ íŒŒì¼ í™•ì¸
        if not Path(data_path).exists():
            print(f"âŒ Input data not found: {data_path}")
            return
        
        # ì‹œìŠ¤í…œ ìƒì„±
        rag_system = create_rag_system(enable_llm=enable_llm)
        
        # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
        train_data_path = './/korean_language_rag_V1.0_train.json'
        if Path(train_data_path).exists():
            rag_system.load_knowledge_base(train_data_path)
        else:
            print(f"âš ï¸ Training data not found: {train_data_path}")
            print("    Creating minimal knowledge base...")
            # ìµœì†Œ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±
            create_minimal_knowledge_base(rag_system)
        
        # ì˜ˆì¸¡ ì‹¤í–‰
        metrics = run_optimized_evaluation(rag_system, data_path, output_path, max_samples=0)
        
        print(f"\nâœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"   ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {metrics['total_samples']}")
        print(f"   í‰ê·  ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {metrics['average_contexts_used']:.2f}")
        print(f"   ê²°ê³¼ ì €ì¥: {output_path}")
        
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        if rag_system:
            rag_system.cleanup()
        MemoryManager.clear_gpu_memory(force=True)

def create_minimal_knowledge_base(rag_system):
    """ìµœì†Œí•œì˜ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±"""
    minimal_chunks = [
        {
            'id': 'chunk_1',
            'text': 'í•œêµ­ì–´ ë§ì¶¤ë²•ì—ì„œ "ë¨¹ì´ì–‘"ì´ ì˜¬ë°”ë¥¸ í‘œí˜„ì…ë‹ˆë‹¤. í•œìì–´ "é‡"ì€ ì•ë§ì´ ê³ ìœ ì–´ì¼ ë•Œ "ì–‘"ì´ ë©ë‹ˆë‹¤.',
            'category': 'ë§ì¶¤ë²•',
            'question_type': 'ì„ íƒí˜•',
            'source': 'minimal'
        },
        {
            'id': 'chunk_2', 
            'text': 'ë„ì–´ì“°ê¸°ì—ì„œ ì˜ì¡´ëª…ì‚¬ëŠ” ì•ë§ê³¼ ë„ì–´ ì¨ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: ì™¸ì¶œí•  ë•Œì—ëŠ”',
            'category': 'ë„ì–´ì“°ê¸°',
            'question_type': 'êµì •í˜•',
            'source': 'minimal'
        },
        {
            'id': 'chunk_3',
            'text': 'ì™¸ë˜ì–´ í‘œê¸°ë²•ì— ë”°ë¼ "ê»Œ"ì´ ì˜¬ë°”ë¥¸ í‘œí˜„ì…ë‹ˆë‹¤. ì˜ì–´ "gum"ì—ì„œ ì˜¨ ì™¸ë˜ì–´ì…ë‹ˆë‹¤.',
            'category': 'ì™¸ë˜ì–´í‘œê¸°',
            'question_type': 'ì„ íƒí˜•', 
            'source': 'minimal'
        }
    ]
    
    rag_system.knowledge_chunks = minimal_chunks
    from utils import HybridRetriever
    rag_system.hybrid_retriever = HybridRetriever(minimal_chunks, None)
    print("âœ… ìµœì†Œ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

def test_mode(enable_llm=True):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ - LLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    print(f"ğŸ§ª Test Mode (LLM: {enable_llm})")
    print("=" * 60)

    # A100 í™˜ê²½ ìµœì í™”
    if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name():
        setup_a100_environment()

    rag_system = None
    try:
        # ì‹œìŠ¤í…œ ìƒì„±
        rag_system = create_rag_system(enable_llm=enable_llm)

        # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
        train_data_path = './/korean_language_rag_V1.0_train.json'
        if Path(train_data_path).exists():
            rag_system.load_knowledge_base(train_data_path)
        else:
            print(f"âš ï¸ Training data not found, creating minimal knowledge base")
            create_minimal_knowledge_base(rag_system)

        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
        test_question = "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤."

        print(f"ğŸ”¬ Testing with question: {test_question}")
        print("-" * 40)

        if enable_llm:
            print("ğŸ¤– Testing LLM components...")

            # ê° LLM ì»´í¬ë„ŒíŠ¸ ì•ˆì „ í…ŒìŠ¤íŠ¸
            try:
                # 1. Query Rewriter í…ŒìŠ¤íŠ¸
                print("\n1. Testing Query Rewriter...")
                if rag_system.query_rewriter:
                    def test_rewriter():
                        return rag_system.query_rewriter.rewrite_query(test_question)
                    
                    expanded = MemoryManager.safe_model_operation(test_rewriter)
                    if expanded:
                        print(f"   Original: {test_question}")
                        print(f"   Expanded: {expanded}")
                    else:
                        print("   âŒ Query Rewriter í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

                # 2. Embedder í…ŒìŠ¤íŠ¸
                print("\n2. Testing Korean Embedder...")
                if rag_system.embedder:
                    def test_embedder():
                        return rag_system.embedder.encode([test_question])
                    
                    embeddings = MemoryManager.safe_model_operation(test_embedder)
                    if embeddings is not None:
                        shape = embeddings.shape if hasattr(embeddings, 'shape') else 'N/A'
                        print(f"   Embedding shape: {shape}")
                    else:
                        print("   âŒ Embedder í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

                # 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
                print("\n3. Testing Full Pipeline...")
                def test_pipeline():
                    return rag_system.process_question(test_question, "ì„ íƒí˜•")
                
                result = MemoryManager.safe_model_operation(test_pipeline)
                if result:
                    answer = result.get('final_answer') or result.get('rankrag_answer') or "No answer"
                    print(f"   Final Answer: {answer}")
                else:
                    print("   âŒ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

            except Exception as e:
                print(f"âš ï¸ LLM test failed: {e}")
                print("   This is expected if models are not available in this environment")

        else:
            print("ğŸ“ Testing Template Mode...")
            def test_template():
                return rag_system.process_question(test_question, "ì„ íƒí˜•")
            
            result = MemoryManager.safe_model_operation(test_template)
            if result:
                answer = result.get('final_answer') or result.get('rankrag_answer') or "No answer"
                print(f"   Template Answer: {answer}")
            else:
                print("   âŒ í…œí”Œë¦¿ ëª¨ë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

        print(f"\nâœ… Test completed!")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        if rag_system:
            rag_system.cleanup()
        MemoryManager.clear_gpu_memory(force=True)

def show_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ (A100 ì •ë³´ í¬í•¨)"""
    print("ğŸ’» System Information")
    print("=" * 60)

    # GPU ì •ë³´ (ìƒì„¸)
    MemoryManager.check_memory_status()
    
    # CUDA ì •ë³´
    try:
        import torch
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name()
            print(f"\nğŸ® GPU ì •ë³´:")
            print(f"   ì¥ì¹˜ëª…: {device_name}")
            print(f"   CUDA ë²„ì „: {torch.version.cuda}")
            print(f"   ì¥ì¹˜ ìˆ˜: {torch.cuda.device_count()}")
            
            if 'A100' in device_name:
                print(f"   âœ… A100 ìµœì í™” ì‚¬ìš© ê°€ëŠ¥!")
        else:
            print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
    except Exception as e:
        print(f"âŒ GPU ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")

    # ë°ì´í„° íŒŒì¼ í™•ì¸
    data_files = [
        './/korean_language_rag_V1.0_train.json',
        './/korean_language_rag_V1.0_dev.json', 
        './/korean_language_rag_V1.0_test.json'
    ]

    print("\nğŸ“‚ Data Files:")
    for file_path in data_files:
        if Path(file_path).exists():
            size = Path(file_path).stat().st_size / 1024  # KB
            print(f"   âœ… {Path(file_path).name} ({size:.1f} KB)")
        else:
            print(f"   âŒ {Path(file_path).name} (not found)")

    # ì˜ì¡´ì„± í™•ì¸
    print("\nğŸ“¦ Dependencies:")
    dependencies = [
        ('torch', 'PyTorch'),
        ('transformers', 'Transformers'),
        ('sentence_transformers', 'Sentence Transformers'),
        ('sklearn', 'Scikit-learn'),
        ('numpy', 'NumPy')
    ]
    
    for module_name, display_name in dependencies:
        try:
            module = __import__(module_name)
            version = getattr(module, '__version__', 'Unknown')
            print(f"   âœ… {display_name}: {version}")
        except ImportError:
            print(f"   âŒ {display_name} not available")

def main():
    """ë©”ì¸ í•¨ìˆ˜ (A100 ìµœì í™”)"""
    parser = argparse.ArgumentParser(
        description="Korean Grammar RAG System (A100 Optimized)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python main.py --mode demo
    python main.py --mode evaluate --samples 5 --enable_llm
    python main.py --mode predict --data_path test.json --output_path predictions.json
    python main.py --mode test --enable_llm
    python main.py --mode info
        """
    )

    parser.add_argument(
        '--mode',
        choices=['demo', 'evaluate', 'test', 'predict', 'info'],
        default='demo',
        help='Execution mode'
    )

    parser.add_argument(
        '--samples',
        type=int,
        default=10,
        help='Number of samples for evaluation (0 = all, default: 10)'
    )

    parser.add_argument(
        '--enable_llm',
        action='store_true',
        help='Enable LLM models (requires GPU and model downloads)'
    )
    
    parser.add_argument(
        '--data_path',
        type=str,
        help='Input data path for prediction mode'
    )
    
    parser.add_argument(
        '--output_path', 
        type=str,
        help='Output path for prediction results'
    )

    args = parser.parse_args()

    print("ğŸ‡°ğŸ‡· Korean Grammar RAG System (A100 Optimized)")
    print("=" * 60)

    try:
        if args.mode == 'demo':
            demo_mode()
        elif args.mode == 'evaluate':
            evaluate_mode(args.samples, args.enable_llm)
        elif args.mode == 'test':
            test_mode(args.enable_llm)
        elif args.mode == 'predict':
            if not args.data_path or not args.output_path:
                print("âŒ predict ëª¨ë“œëŠ” --data_pathì™€ --output_pathê°€ í•„ìš”í•©ë‹ˆë‹¤")
                return
            predict_mode(args.data_path, args.output_path, args.enable_llm)
        elif args.mode == 'info':
            show_system_info()

    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        traceback.print_exc()
    finally:
        # ìµœì¢… ì •ë¦¬
        MemoryManager.clear_gpu_memory(force=True)
        print("\nğŸ§¹ System cleanup completed")

if __name__ == "__main__":
    main()
