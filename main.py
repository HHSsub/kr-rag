"""
Korean Grammar RAG System - Main Execution Script
í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” RAG ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python main.py --mode demo                    # ë°ëª¨ ì‹¤í–‰
    python main.py --mode evaluate --samples 10  # í‰ê°€ ì‹¤í–‰
    python main.py --mode test --enable_llm      # LLM í™œì„±í™” í…ŒìŠ¤íŠ¸
"""

import argparse
import json
import time
from pathlib import Path

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from rag_pipeline import KoreanGrammarRAGSystem, create_rag_system, quick_test
from utils import DataLoader, MemoryManager

def demo_mode():
    """ë°ëª¨ ëª¨ë“œ - ëª‡ ê°œ ìƒ˜í”Œ ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ­ Demo Mode - Korean Grammar RAG System")
    print("=" * 60)

    # ì‹œìŠ¤í…œ ìƒì„± (LLM ë¹„í™œì„±í™”ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    print("ğŸš€ Creating RAG System (Template Mode)...")
    rag_system = create_rag_system(enable_llm=False)

    # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    train_data_path = './/korean_language_rag_V1.0_train.json'
    if not Path(train_data_path).exists():
        print(f"âŒ Training data not found: {train_data_path}")
        return

    rag_system.load_knowledge_base(train_data_path)

    # ë°ëª¨ ì§ˆë¬¸ë“¤
    demo_questions = [
        {
            "question": "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
            "type": "ì„ íƒí˜•"
        },
        {
            "question": "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\"ì™¸ì¶œì‹œì—ëŠ” ì—ì–´ì»¨ì„ ê¼­ ë•ì‹œë‹¤.\"",
            "type": "êµì •í˜•"
        },
        {
            "question": "{ê²€/ê»Œ}ì„ ì”¹ë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
            "type": "ì„ íƒí˜•"
        }
    ]

    # ê° ì§ˆë¬¸ ì²˜ë¦¬
    for i, demo in enumerate(demo_questions, 1):
        print(f"\nğŸ“ Demo Question {i}: {demo['type']}")
        print(f"Q: {demo['question']}")
        print("-" * 40)

        start_time = time.time()
        result = rag_system.process_question(demo['question'], demo['type'])
        processing_time = time.time() - start_time

        print(f"A: {result['final_answer'] or result['rankrag_answer']}")
        print(f"â±ï¸  Processing time: {processing_time:.2f}s")
        print(f"ğŸ“Š Contexts used: {len(result['reranked_contexts'])}")

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.clear_gpu_memory()

    print(f"\nâœ… Demo completed successfully!")
    rag_system.cleanup()

def evaluate_mode(max_samples=10, enable_llm=False):
    """í‰ê°€ ëª¨ë“œ - ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í‰ê°€"""
    print(f"ğŸ“Š Evaluation Mode (LLM: {enable_llm}, Samples: {max_samples})")
    print("=" * 60)

    # ì‹œìŠ¤í…œ ìƒì„±
    rag_system = create_rag_system(enable_llm=enable_llm)

    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    train_data_path = './/korean_language_rag_V1.0_train.json'
    dev_data_path = './/korean_language_rag_V1.0_dev.json'

    if not Path(train_data_path).exists():
        print(f"âŒ Training data not found: {train_data_path}")
        return

    if not Path(dev_data_path).exists():
        print(f"âŒ Development data not found: {dev_data_path}")
        return

    # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    rag_system.load_knowledge_base(train_data_path)

    # í‰ê°€ ì‹¤í–‰
    output_path = f'.//evaluation_results_llm_{enable_llm}.json'

    start_time = time.time()
    metrics = rag_system.evaluate_on_dataset(
        dev_data_path, 
        output_path=output_path,
        max_samples=max_samples
    )
    total_time = time.time() - start_time

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ Evaluation Results:")
    print(f"   Accuracy: {metrics['accuracy']:.4f}")
    print(f"   Correct: {metrics['correct_predictions']}/{metrics['total_samples']}")
    print(f"   Avg Contexts: {metrics['average_contexts_used']:.2f}")
    print(f"   Total Time: {total_time:.2f}s")
    print(f"   Time per Sample: {total_time/metrics['total_samples']:.2f}s")

    rag_system.cleanup()

def test_mode(enable_llm=True):
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ - LLM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§ª Test Mode (LLM: {enable_llm})")
    print("=" * 60)

    # ì‹œìŠ¤í…œ ìƒì„±
    rag_system = create_rag_system(enable_llm=enable_llm)

    # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    train_data_path = './/korean_language_rag_V1.0_train.json'
    if Path(train_data_path).exists():
        rag_system.load_knowledge_base(train_data_path)
    else:
        print(f"âš ï¸ Training data not found, creating minimal knowledge base")
        # ìµœì†Œí•œì˜ ì§€ì‹ ë² ì´ìŠ¤ ìƒì„±
        rag_system.knowledge_chunks = [
            {
                'id': 'test_chunk_1',
                'text': 'í•œêµ­ì–´ ë§ì¶¤ë²•ì—ì„œ "ë¨¹ì´ì–‘"ì´ ì˜¬ë°”ë¥¸ í‘œí˜„ì…ë‹ˆë‹¤. í•œìì–´ "é‡"ì€ ì•ë§ì´ ê³ ìœ ì–´ì¼ ë•Œ "ì–‘"ì´ ë©ë‹ˆë‹¤.',
                'category': 'ë§ì¶¤ë²•',
                'question_type': 'ì„ íƒí˜•',
                'source': 'test'
            }
        ]
        from utils import HybridRetriever
        rag_system.hybrid_retriever = HybridRetriever(rag_system.knowledge_chunks, None)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤."

    print(f"ğŸ”¬ Testing with question: {test_question}")
    print("-" * 40)

    if enable_llm:
        print("ğŸ¤– Testing LLM components...")

        # ê° LLM ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
        try:
            # 1. Query Rewriter í…ŒìŠ¤íŠ¸
            print("\n1. Testing Query Rewriter...")
            if rag_system.query_rewriter:
                expanded = rag_system.query_rewriter.rewrite_query(test_question)
                print(f"   Original: {test_question}")
                print(f"   Expanded: {expanded}")

            # 2. Embedder í…ŒìŠ¤íŠ¸
            print("\n2. Testing Korean Embedder...")
            if rag_system.embedder:
                embeddings = rag_system.embedder.encode([test_question])
                print(f"   Embedding shape: {embeddings.shape if hasattr(embeddings, 'shape') else 'N/A'}")

            # 3. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
            print("\n3. Testing Full Pipeline...")
            result = rag_system.process_question(test_question, "ì„ íƒí˜•")
            print(f"   Final Answer: {result['final_answer'] or result['rankrag_answer']}")

        except Exception as e:
            print(f"âš ï¸ LLM test failed: {e}")
            print("   This is expected if models are not available in this environment")

    else:
        print("ğŸ“ Testing Template Mode...")
        result = rag_system.process_question(test_question, "ì„ íƒí˜•")
        print(f"   Template Answer: {result['final_answer'] or result['rankrag_answer']}")

    print(f"\nâœ… Test completed!")
    rag_system.cleanup()

def show_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ"""
    print("ğŸ’» System Information")
    print("=" * 60)

    # GPU ì •ë³´
    gpu_info = MemoryManager.get_gpu_memory_info()
    print(f"GPU: {gpu_info}")

    # ë°ì´í„° íŒŒì¼ í™•ì¸
    data_files = [
        './/korean_language_rag_V1.0_train.json',
        './/korean_language_rag_V1.0_dev.json',
        './/korean_language_rag_V1.0_test.json'
    ]

    print("\nğŸ“‚ Data Files:")
    for file_path in data_files:
        if Path(file_path).exists():
            size = Path(file_path).stat().st_size / 1024  # KB
            print(f"   âœ… {Path(file_path).name} ({size:.1f} KB)")
        else:
            print(f"   âŒ {Path(file_path).name} (not found)")

    # ì˜ì¡´ì„± í™•ì¸
    print("\nğŸ“¦ Dependencies:")
    try:
        import torch
        print(f"   âœ… PyTorch: {torch.__version__}")
        print(f"   âœ… CUDA Available: {torch.cuda.is_available()}")
    except ImportError:
        print("   âŒ PyTorch not available")

    try:
        import transformers
        print(f"   âœ… Transformers: {transformers.__version__}")
    except ImportError:
        print("   âŒ Transformers not available")

    try:
        import sentence_transformers
        print(f"   âœ… Sentence Transformers: {sentence_transformers.__version__}")
    except ImportError:
        print("   âŒ Sentence Transformers not available")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Korean Grammar RAG System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python main.py --mode demo
    python main.py --mode evaluate --samples 5
    python main.py --mode test --enable_llm
    python main.py --mode info
        """
    )

    parser.add_argument(
        '--mode',
        choices=['demo', 'evaluate', 'test', 'info'],
        default='demo',
        help='Execution mode'
    )

    parser.add_argument(
        '--samples',
        type=int,
        default=10,
        help='Number of samples for evaluation (default: 10)'
    )

    parser.add_argument(
        '--enable_llm',
        action='store_true',
        help='Enable LLM models (requires GPU and model downloads)'
    )

    args = parser.parse_args()

    print("ğŸ‡°ğŸ‡· Korean Grammar RAG System")
    print("=" * 60)

    try:
        if args.mode == 'demo':
            demo_mode()
        elif args.mode == 'evaluate':
            evaluate_mode(args.samples, args.enable_llm)
        elif args.mode == 'test':
            test_mode(args.enable_llm)
        elif args.mode == 'info':
            show_system_info()

    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ìµœì¢… ì •ë¦¬
        MemoryManager.clear_gpu_memory()
        print("\nğŸ§¹ System cleanup completed")

if __name__ == "__main__":
    main()
