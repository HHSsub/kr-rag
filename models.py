"""
Korean Grammar RAG System - LLM Model Wrappers (A100 ìµœì í™” ë²„ì „)
íƒœìŠ¤í¬ë³„ ìµœì í™”ëœ Hugging Face ëª¨ë¸ë“¤ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë“¤
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

# A100 ìµœì í™”ë¥¼ ìœ„í•œ imports
try:
    from utils import MemoryManager
except ImportError:
    class MemoryManager:
        @staticmethod
        def check_memory_status():
            return {'usage_ratio': 0.5}
        @staticmethod
        def clear_gpu_memory():
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

# bitsandbytes ì™„ì „ ë¹„í™œì„±í™”
BITSANDBYTES_AVAILABLE = False

class ModelConfig:
    """A100 ìµœì í™” ëª¨ë¸ ì„¤ì • ê´€ë¦¬"""
    
    def __init__(self):
        # A100 ìµœì í™” ì„¤ì •
        self.device_map = "auto"
        self.torch_dtype = torch.float16  # bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
        self.low_cpu_mem_usage = True
        self.load_in_8bit = True  # 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©
        self.max_memory = {0: "35GB"}  # A100ì˜ 80% ì‚¬ìš©
        self.trust_remote_code = True
    
    # PyTorch ë°ì´í„° íƒ€ì…
    TORCH_DTYPE = torch.float16
    
    # ì–‘ìí™” ì„¤ì • - ì™„ì „íˆ ë¹„í™œì„±í™”
    QUANTIZATION_CONFIG = None
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    GENERATION_CONFIG = {
        "max_new_tokens": 512,
        "temperature": 0.7,
        "do_sample": True,
        "pad_token_id": None,  # ëª¨ë¸ë³„ë¡œ ì„¤ì •
        "eos_token_id": None,  # ëª¨ë¸ë³„ë¡œ ì„¤ì •
    }

class QueryRewriter:
    """ì¿¼ë¦¬ ì¬ì‘ì„± ë° HyDE êµ¬í˜„"""

    def __init__(self):
        self.model_name = "MLP-KTLim/llama-3-Korean-Bllossom-8B"
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        self.config = ModelConfig()

    def load_model(self):
        """A100 ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        if self.is_loaded:
            return

        print(f"ğŸ”„ Loading Query Rewriter: {self.model_name}")

        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            MemoryManager.check_memory_status()
            
            # ì´ì „ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            MemoryManager.clear_gpu_memory()

            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=self.config.device_map,
                torch_dtype=self.config.torch_dtype,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                load_in_8bit=self.config.load_in_8bit,
                max_memory=self.config.max_memory,
                trust_remote_code=self.config.trust_remote_code
            )

            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.is_loaded = True
            print(f"âœ… {self.model_name} ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            MemoryManager.clear_gpu_memory()

    def rewrite_query(self, question):
        """ì¿¼ë¦¬ ì¬ì‘ì„± ë° í™•ì¥"""
        if not self.is_loaded:
            self.load_model()
            
        if not self.is_loaded:
            return question  # ë¡œë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

        prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ í™•ì¥í•´ ì£¼ì„¸ìš”.
ê°€ëŠ¥í•œ í‘œí˜„ì„ ì¤‘ê´„í˜¸ {{ì„ íƒ1/ì„ íƒ2}} í˜•ì‹ìœ¼ë¡œ ë¬¶ì–´ ì¶œë ¥í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

í™•ì¥ëœ ì§ˆë¬¸:"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ì›ë˜ í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
            expanded_query = response.split("í™•ì¥ëœ ì§ˆë¬¸:")[-1].strip()

            return expanded_query if expanded_query else question
            
        except Exception as e:
            print(f"âŒ Query rewriting ì‹¤íŒ¨: {e}")
            return question

class KoreanEmbedder:
    """í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©"""

    def __init__(self):
        self.model_name = "jhgan/ko-sbert-sts"
        self.model = None
        self.is_loaded = False
        self.config = ModelConfig()

    def load_model(self):
        """A100 ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        if self.is_loaded:
            return

        print(f"ğŸ”„ Loading Korean Embedder: {self.model_name}")
        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            MemoryManager.check_memory_status()
            
            # ì´ì „ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            MemoryManager.clear_gpu_memory()
            
            self.model = SentenceTransformer(self.model_name)
            self.is_loaded = True
            print(f"âœ… {self.model_name} ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            MemoryManager.clear_gpu_memory()

    def encode(self, texts):
        """í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        if not self.is_loaded:
            self.load_model()
            
        if not self.is_loaded:
            # ì„ë² ë”© ì‹¤íŒ¨ ì‹œ ëœë¤ ë²¡í„° ë°˜í™˜
            import numpy as np
            if isinstance(texts, str):
                return torch.tensor(np.random.random((1, 768)))
            return torch.tensor(np.random.random((len(texts), 768)))

        if isinstance(texts, str):
            texts = [texts]

        try:
            embeddings = self.model.encode(texts, convert_to_tensor=True)
            return embeddings
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            import numpy as np
            return torch.tensor(np.random.random((len(texts), 768)))

class RankRAGModel:
    """RankRAG: ì»¨í…ìŠ¤íŠ¸ ë­í‚¹ + ë‹µë³€ ìƒì„± í†µí•©"""

    def __init__(self):
        self.model_name = "dnotitia/Llama-DNA-1.0-8B-Instruct"
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        self.config = ModelConfig()

    def load_model(self):
        """A100 ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        if self.is_loaded:
            return

        print(f"ğŸ”„ Loading RankRAG Model: {self.model_name}")

        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            MemoryManager.check_memory_status()
            
            # ì´ì „ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            MemoryManager.clear_gpu_memory()

            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=self.config.device_map,
                torch_dtype=self.config.torch_dtype,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                load_in_8bit=self.config.load_in_8bit,
                max_memory=self.config.max_memory,
                trust_remote_code=self.config.trust_remote_code
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.is_loaded = True
            print(f"âœ… {self.model_name} ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            MemoryManager.clear_gpu_memory()

    def rank_and_generate(self, question, contexts, question_type="ì„ íƒí˜•"):
        """ì»¨í…ìŠ¤íŠ¸ ë­í‚¹ + ë‹µë³€ ìƒì„±"""
        if not self.is_loaded:
            self.load_model()
            
        if not self.is_loaded:
            # ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
            return f'"{question}ì— ëŒ€í•œ ë‹µë³€"ì´ ì˜³ë‹¤. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ë‹µë³€ì…ë‹ˆë‹¤.'

        # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context_text = ""
        for i, ctx in enumerate(contexts[:5], 1):  # ìµœëŒ€ 5ê°œ ì»¨í…ìŠ¤íŠ¸
            context_text += f"{i}. {ctx.get('text', str(ctx))[:500]}...\n\n"

        prompt = f"""í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

ì§ˆë¬¸ ìœ í˜•: {question_type}
ì§ˆë¬¸: {question}

ì°¸ì¡° ì»¨í…ìŠ¤íŠ¸:
{context_text}

ê° ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ë„ë¥¼ í‰ê°€í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

ë‹µë³€ í˜•ì‹: "{{ì •ë‹µ}}ì´/ê°€ ì˜³ë‹¤. {{ìƒì„¸í•œ ì´ìœ ì™€ ì„¤ëª…}}"

ë‹µë³€:"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=2048)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.3,  # ì •í™•ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response.split("ë‹µë³€:")[-1].strip()

            return answer if answer else "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"âŒ RankRAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f'"{question}ì— ëŒ€í•œ ë‹µë³€"ì´ ì˜³ë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'

class GuidedRankSelector:
    """LLM Guided Rank Selection - ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ì„¤ëª…"""

    def __init__(self):
        self.model_name = "KRAFTON/KORani-v3-13B"
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        self.config = ModelConfig()

    def load_model(self):
        """A100 ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        if self.is_loaded:
            return

        print(f"ğŸ”„ Loading Guided Rank Selector: {self.model_name}")

        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            MemoryManager.check_memory_status()
            
            # ì´ì „ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            MemoryManager.clear_gpu_memory()

            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=self.config.device_map,
                torch_dtype=self.config.torch_dtype,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                load_in_8bit=self.config.load_in_8bit,
                max_memory=self.config.max_memory,
                trust_remote_code=self.config.trust_remote_code
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.is_loaded = True
            print(f"âœ… {self.model_name} ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            MemoryManager.clear_gpu_memory()

    def explain_context_ranking(self, question, contexts):
        """ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ì„¤ëª… ìƒì„±"""
        if not self.is_loaded:
            self.load_model()
            
        if not self.is_loaded:
            return "ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•´ ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        context_list = ""
        for i, ctx in enumerate(contexts[:3], 1):  # ìµœëŒ€ 3ê°œ ë¶„ì„
            context_list += f"{i}. {ctx.get('text', str(ctx))[:300]}...\n\n"

        prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì»¨í…ìŠ¤íŠ¸ ëª©ë¡:
{context_list}

ê° ì»¨í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”:

í‰ê°€ ê¸°ì¤€:
- ì§ˆë¬¸ê³¼ì˜ ì§ì ‘ì  ê´€ë ¨ì„±
- ì–´ë¬¸ ê·œë²” ì§€ì‹ì˜ ì •í™•ì„±
- ë‹µë³€ ìƒì„±ì— í•„ìš”í•œ ì •ë³´ í¬í•¨ë„

ì¶œë ¥ í˜•ì‹:
ì»¨í…ìŠ¤íŠ¸ 1 - ì¤‘ìš”ë„: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ì´ìœ : [êµ¬ì²´ì  ì„¤ëª…]
ì»¨í…ìŠ¤íŠ¸ 2 - ì¤‘ìš”ë„: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ì´ìœ : [êµ¬ì²´ì  ì„¤ëª…]
ì»¨í…ìŠ¤íŠ¸ 3 - ì¤‘ìš”ë„: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ì´ìœ : [êµ¬ì²´ì  ì„¤ëª…]

í‰ê°€ ê²°ê³¼:"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1536)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=400,
                    temperature=0.5,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            explanation = response.split("í‰ê°€ ê²°ê³¼:")[-1].strip()

            return explanation if explanation else "ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"âŒ ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

class FinalAnswerGenerator:
    """ìµœì¢… ë‹µë³€ ë° ì„¤ëª… ìƒì„±"""

    def __init__(self):
        self.model_name = "yanolja/EEVE-Korean-10.8B-v1.0"
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        self.config = ModelConfig()

    def load_model(self):
        """A100 ìµœì í™” ëª¨ë¸ ë¡œë”©"""
        if self.is_loaded:
            return

        print(f"ğŸ”„ Loading Final Answer Generator: {self.model_name}")

        try:
            # ë©”ëª¨ë¦¬ ì²´í¬
            MemoryManager.check_memory_status()
            
            # ì´ì „ ëª¨ë¸ ì •ë¦¬
            if hasattr(self, 'model') and self.model is not None:
                del self.model
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
            MemoryManager.clear_gpu_memory()

            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=self.config.device_map,
                torch_dtype=self.config.torch_dtype,
                low_cpu_mem_usage=self.config.low_cpu_mem_usage,
                load_in_8bit=self.config.load_in_8bit,
                max_memory=self.config.max_memory,
                trust_remote_code=self.config.trust_remote_code
            )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.is_loaded = True
            print(f"âœ… {self.model_name} ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            MemoryManager.clear_gpu_memory()

    def generate_final_answer(self, question, question_type, selected_contexts, context_explanation):
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        if not self.is_loaded:
            self.load_model()
            
        if not self.is_loaded:
            return f'"{question}ì— ëŒ€í•œ ë‹µë³€"ì´ ì˜³ë‹¤. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ë‹µë³€ì…ë‹ˆë‹¤.'

        contexts_text = ""
        for i, ctx in enumerate(selected_contexts, 1):
            contexts_text += f"- {ctx.get('text', str(ctx))[:200]}...\n"

        prompt = f"""í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸ ìœ í˜•: {question_type}
ì§ˆë¬¸: {question}

ì°¸ì¡°í•œ ê·œë²” ì§€ì‹:
{contexts_text}

ì»¨í…ìŠ¤íŠ¸ ë¶„ì„:
{context_explanation}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:

1. ì •ë‹µ: "{{ì •í™•í•œ ì •ë‹µ}}ì´/ê°€ ì˜³ë‹¤."
2. ê·œë²” ê·¼ê±°: {{í•´ë‹¹ ì–´ë¬¸ ê·œë²” ì¡°í•­ê³¼ ì›ì¹™}}
3. ìƒì„¸ ì„¤ëª…: {{ë¬¸ë²•ì  ê·¼ê±°ì™€ ë…¼ë¦¬ì  ì„¤ëª…}}
4. ì˜ˆì‹œ: {{ì ì ˆí•œ ì˜ˆì‹œ 2-3ê°œ}}
5. ì£¼ì˜ì‚¬í•­: {{ìì£¼ í‹€ë¦¬ëŠ” í‘œí˜„ì´ë‚˜ í˜¼ë™ ì‚¬ë¡€}}

ë‹µë³€:"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=2048)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=600,
                    temperature=0.4,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response.split("ë‹µë³€:")[-1].strip()

            return answer if answer else "ìµœì¢… ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"âŒ ìµœì¢… ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f'"{question}ì— ëŒ€í•œ ë‹µë³€"ì´ ì˜³ë‹¤. ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìœ í‹¸ë¦¬í‹°
class PromptTemplates:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬"""

    @staticmethod
    def get_prompt(stage, **kwargs):
        """ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if stage == "rewrite":
            return f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ í™•ì¥í•´ ì£¼ì„¸ìš”:
ì§ˆë¬¸: {kwargs['question']}

í™•ì¥ëœ í‘œí˜„:"""

        elif stage == "rankrag":
            contexts = "\n".join([f"{i+1}. {ctx}" for i, ctx in enumerate(kwargs['contexts'])])
            return f"""ì§ˆë¬¸: {kwargs['question']}

{contexts}

ê°€ì¥ ì¤‘ìš”í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."""

        elif stage == "guided_rank":
            contexts = "\n".join([f"{i+1}. {ctx}" for i, ctx in enumerate(kwargs['contexts'])])
            return f"""ì§ˆë¬¸: {kwargs['question']}

ê° ì»¨í…ìŠ¤íŠ¸ì˜ ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ê³  ì„¤ëª…í•˜ì„¸ìš”:
{contexts}"""

        elif stage == "final":
            return f"""ì§ˆë¬¸: {kwargs['question']}

ì •í™•í•œ ê·œë²” ê¸°ë°˜ ì„¤ëª…ê³¼ ì˜ˆì‹œë¥¼ í¬í•¨í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."""

        return ""
