"""
Korean Grammar RAG System - Utility Functions
í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import re
import json
import numpy as np
from typing import List, Dict, Any, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import torch

class KoreanTextProcessor:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""

    @staticmethod
    def extract_options_from_question(question: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ ì„ íƒì§€ ì¶”ì¶œ (ì˜ˆ: {ì˜µì…˜1/ì˜µì…˜2})"""
        pattern = r'{([^}]+)}'
        matches = re.findall(pattern, question)

        options = []
        for match in matches:
            if '/' in match:
                options.extend([opt.strip() for opt in match.split('/')])

        return options

    @staticmethod
    def expand_query_with_options(question: str) -> List[str]:
        """ì§ˆë¬¸ì„ ì„ íƒì§€ë¡œ í™•ì¥"""
        options = KoreanTextProcessor.extract_options_from_question(question)
        expanded_queries = [question]

        # ê° ì„ íƒì§€ë¡œ ì§ˆë¬¸ í™•ì¥
        for option in options:
            # ì¤‘ê´„í˜¸ ë¶€ë¶„ì„ ê° ì„ íƒì§€ë¡œ ëŒ€ì²´
            pattern = r'{[^}]+}'
            expanded_query = re.sub(pattern, option, question)
            if expanded_query not in expanded_queries:
                expanded_queries.append(expanded_query)

        return expanded_queries

    @staticmethod
    def extract_grammar_keywords(text: str) -> List[str]:
        """ë¬¸ë²• ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        grammar_keywords = [
            'ë§ì¶¤ë²•', 'ë„ì–´ì“°ê¸°', 'í‘œì¤€ì–´', 'ë¬¸ì¥ë¶€í˜¸', 'ì™¸ë˜ì–´í‘œê¸°',
            'ì–´ê°„', 'ì–´ë¯¸', 'ë°›ì¹¨', 'í™œìš©', 'ì¡°ì‚¬', 'ì˜ì¡´ëª…ì‚¬',
            'ì–‘ì„±ëª¨ìŒ', 'ìŒì„±ëª¨ìŒ', 'ë‘ìŒë²•ì¹™', 'ì‚¬ì´ì‹œì˜·',
            'ë§ˆì¹¨í‘œ', 'ì‰¼í‘œ', 'ë¬¼ìŒí‘œ', 'ëŠë‚Œí‘œ', 'ê´„í˜¸', 'ë”°ì˜´í‘œ'
        ]

        found_keywords = []
        for keyword in grammar_keywords:
            if keyword in text:
                found_keywords.append(keyword)

        return found_keywords

    @staticmethod
    def normalize_korean_text(text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())

        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬ (í•„ìš”í•œ ê²ƒë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£{}/.,;:!?""''()\[\]-]', '', text)

        return text

class HybridRetriever:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + Sparse)"""

    def __init__(self, knowledge_chunks: List[Dict], embedder=None):
        self.knowledge_chunks = knowledge_chunks
        self.embedder = embedder
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.chunk_embeddings = None

        self._build_indices()

    def _build_indices(self):
        """ê²€ìƒ‰ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            # TF-IDF ì¸ë±ìŠ¤ êµ¬ì¶• (Sparse)
            texts = [chunk['text'] for chunk in self.knowledge_chunks]
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=None
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)

            # Dense ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• (ì„ë² ë”ê°€ ìˆëŠ” ê²½ìš°)
            if self.embedder:
                print("ğŸ”„ Building dense embeddings for knowledge chunks...")
                self.chunk_embeddings = self.embedder.encode(texts)
                print("âœ… Dense embeddings built successfully")
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨: {e}")

    def sparse_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """TF-IDF ê¸°ë°˜ sparse ê²€ìƒ‰"""
        try:
            query_vector = self.tfidf_vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()

            # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ì™€ ì ìˆ˜ ë°˜í™˜
            top_indices = np.argsort(similarities)[::-1][:top_k]
            results = [(idx, similarities[idx]) for idx in top_indices if similarities[idx] > 0]

            return results
        except Exception as e:
            print(f"âŒ Sparse ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def dense_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """Dense ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰"""
        if not self.embedder or self.chunk_embeddings is None:
            return []

        try:
            query_embedding = self.embedder.encode([query])

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            if isinstance(self.chunk_embeddings, torch.Tensor):
                similarities = torch.cosine_similarity(
                    query_embedding, self.chunk_embeddings, dim=1
                ).cpu().numpy()
            else:
                similarities = cosine_similarity(query_embedding, self.chunk_embeddings).flatten()

            # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ì™€ ì ìˆ˜ ë°˜í™˜
            top_indices = np.argsort(similarities)[::-1][:top_k]
            results = [(idx, similarities[idx]) for idx in top_indices if similarities[idx] > 0]

            return results
        except Exception as e:
            print(f"âŒ Dense ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def hybrid_search(self, query: str, top_k: int = 10, 
                     sparse_weight: float = 0.3, dense_weight: float = 0.7) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Sparse + Dense ê²°í•©)"""
        # Sparse ê²€ìƒ‰
        sparse_results = self.sparse_search(query, top_k * 2)

        # Dense ê²€ìƒ‰
        dense_results = self.dense_search(query, top_k * 2)

        # ì ìˆ˜ ì •ê·œí™” ë° ê²°í•©
        combined_scores = {}

        # Sparse ì ìˆ˜ ì¶”ê°€
        for idx, score in sparse_results:
            combined_scores[idx] = sparse_weight * score

        # Dense ì ìˆ˜ ì¶”ê°€
        for idx, score in dense_results:
            if idx in combined_scores:
                combined_scores[idx] += dense_weight * score
            else:
                combined_scores[idx] = dense_weight * score

        # ìµœì¢… ë­í‚¹
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)

        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        final_results = []
        for idx, score in sorted_results[:top_k]:
            chunk = self.knowledge_chunks[idx].copy()
            chunk['retrieval_score'] = score
            final_results.append(chunk)

        return final_results

class MultiStageReranker:
    """ë‹¤ë‹¨ê³„ ì¬ë­í‚¹ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.category_weights = {
            'ë§ì¶¤ë²•': 1.2,
            'ë„ì–´ì“°ê¸°': 1.1,
            'í‘œì¤€ì–´': 1.0,
            'ë¬¸ì¥ë¶€í˜¸': 0.9,
            'ì™¸ë˜ì–´í‘œê¸°': 0.8,
            'ë¬¸ë²•': 1.0
        }

    def calculate_category_match_score(self, question: str, context: Dict) -> float:
        """ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        try:
            question_keywords = KoreanTextProcessor.extract_grammar_keywords(question)
            context_category = context.get('category', '')
            context_keywords = KoreanTextProcessor.extract_grammar_keywords(context['text'])

            # ì¹´í…Œê³ ë¦¬ ì§ì ‘ ë§¤ì¹­
            category_score = 0.0
            if context_category in question or any(kw in context_category for kw in question_keywords):
                category_score = self.category_weights.get(context_category, 1.0)

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_score = len(set(question_keywords) & set(context_keywords)) * 0.1

            return category_score + keyword_score
        except Exception as e:
            print(f"âŒ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_question_type_score(self, question_type: str, context: Dict) -> float:
        """ì§ˆë¬¸ ìœ í˜• ë§¤ì¹­ ì ìˆ˜"""
        try:
            if question_type == 'ì„ íƒí˜•':
                # ì„ íƒí˜• ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¤‘ì¹˜
                if any(word in context['text'] for word in ['ì„ íƒ', 'ì˜³ë‹¤', 'ë°”ë¥´ë‹¤', 'ì˜¬ë°”ë¥¸']):
                    return 0.2
            elif question_type == 'êµì •í˜•':
                # êµì •í˜• ì§ˆë¬¸ì— ëŒ€í•œ ê°€ì¤‘ì¹˜
                if any(word in context['text'] for word in ['êµì •', 'ê³ ì¹˜', 'ë°”ê¾¸', 'ìˆ˜ì •']):
                    return 0.2

            return 0.0
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ìœ í˜• ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_keyword_frequency_score(self, question: str, context: Dict) -> float:
        """í‚¤ì›Œë“œ ë¹ˆë„ ì ìˆ˜"""
        try:
            question_words = set(question.split())
            context_words = context['text'].split()

            common_words = question_words & set(context_words)
            if not question_words:
                return 0.0

            frequency_score = len(common_words) / len(question_words)
            return frequency_score * 0.3
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ë¹ˆë„ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def rerank_contexts(self, question: str, question_type: str, contexts: List[Dict]) -> List[Dict]:
        """ë‹¤ë‹¨ê³„ ì¬ë­í‚¹ ìˆ˜í–‰"""
        reranked_contexts = []

        for context in contexts:
            # ê¸°ë³¸ ê²€ìƒ‰ ì ìˆ˜
            base_score = context.get('retrieval_score', 0.0)

            # ì¶”ê°€ ì ìˆ˜ ê³„ì‚°
            category_score = self.calculate_category_match_score(question, context)
            type_score = self.calculate_question_type_score(question_type, context)
            keyword_score = self.calculate_keyword_frequency_score(question, context)

            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = base_score + category_score + type_score + keyword_score

            context_copy = context.copy()
            context_copy['final_score'] = final_score
            context_copy['category_score'] = category_score
            context_copy['type_score'] = type_score
            context_copy['keyword_score'] = keyword_score

            reranked_contexts.append(context_copy)

        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        reranked_contexts.sort(key=lambda x: x['final_score'], reverse=True)

        return reranked_contexts

class EvaluationMetrics:
    """í‰ê°€ ì§€í‘œ ê³„ì‚°"""

    @staticmethod
    def exact_match(predicted: str, ground_truth: str) -> bool:
        """ì™„ì „ ì¼ì¹˜ í‰ê°€"""
        try:
            # ì •ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë¬¸ì¥)
            pred_answer = predicted.split('.')[0].strip() if '.' in predicted else predicted.strip()
            gt_answer = ground_truth.split('.')[0].strip() if '.' in ground_truth else ground_truth.strip()

            return pred_answer == gt_answer
        except Exception as e:
            print(f"âŒ ì™„ì „ ì¼ì¹˜ í‰ê°€ ì‹¤íŒ¨: {e}")
            return False

    @staticmethod
    def extract_correct_answer(text: str) -> str:
        """ì •ë‹µ ë¶€ë¶„ ì¶”ì¶œ"""
        try:
            # "...ì´/ê°€ ì˜³ë‹¤" íŒ¨í„´ìœ¼ë¡œ ì •ë‹µ ì¶”ì¶œ
            patterns = [
                r'"([^"]+)"[ì´ê°€] ì˜³ë‹¤',
                r"'([^']+)'[ì´ê°€] ì˜³ë‹¤",
                r'([^.]+)[ì´ê°€] ì˜³ë‹¤'
            ]

            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    return match.group(1).strip()

            return text.split('.')[0].strip()
        except Exception as e:
            print(f"âŒ ì •ë‹µ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return text.split('.')[0].strip() if '.' in text else text.strip()

class DataLoader:
    """ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°"""

    @staticmethod
    def load_json_dataset(file_path: str) -> List[Dict]:
        """JSON ë°ì´í„°ì…‹ ë¡œë”©"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            print(f"âŒ JSON ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

    @staticmethod
    def save_results(results: List[Dict], file_path: str):
        """ê²°ê³¼ ì €ì¥"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {file_path}")
        except Exception as e:
            print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    @staticmethod
    def create_knowledge_chunks_from_data(train_data: List[Dict]) -> List[Dict]:
        """í›ˆë ¨ ë°ì´í„°ì—ì„œ ì§€ì‹ ì²­í¬ ìƒì„±"""
        knowledge_chunks = []

        try:
            for i, item in enumerate(train_data):
                question = item['input']['question']
                answer = item['output']['answer']
                question_type = item['input']['question_type']

                # ë‹µë³€ì—ì„œ ê·œë²” ì§€ì‹ ì¶”ì¶œ
                knowledge_text = f"{question} {answer}"

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category = "ê¸°íƒ€"
                if any(word in knowledge_text for word in ['ë§ì¶¤ë²•', 'ì² ì', 'ì–´ê°„', 'ì–´ë¯¸']):
                    category = "ë§ì¶¤ë²•"
                elif any(word in knowledge_text for word in ['ë„ì–´ì“°ê¸°', 'ë„ì–´', 'ë¶™ì—¬']):
                    category = "ë„ì–´ì“°ê¸°"
                elif any(word in knowledge_text for word in ['í‘œì¤€ì–´', 'í‘œì¤€', 'ì‚¬ì •']):
                    category = "í‘œì¤€ì–´"
                elif any(word in knowledge_text for word in ['ë¬¸ì¥ë¶€í˜¸', 'ë§ˆì¹¨í‘œ', 'ì‰¼í‘œ']):
                    category = "ë¬¸ì¥ë¶€í˜¸"
                elif any(word in knowledge_text for word in ['ì™¸ë˜ì–´', 'í‘œê¸°ë²•']):
                    category = "ì™¸ë˜ì–´í‘œê¸°"

                chunk = {
                    'id': f"chunk_{i}",
                    'text': knowledge_text,
                    'category': category,
                    'question_type': question_type,
                    'source': 'training_data'
                }

                knowledge_chunks.append(chunk)

            print(f"âœ… ì§€ì‹ ì²­í¬ ìƒì„± ì™„ë£Œ: {len(knowledge_chunks)}ê°œ")
            return knowledge_chunks
        except Exception as e:
            print(f"âŒ ì§€ì‹ ì²­í¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°"""

    @staticmethod
    def clear_gpu_memory():
        """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print("âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    @staticmethod
    def get_gpu_memory_info():
        """GPU ë©”ëª¨ë¦¬ ì •ë³´ ë°˜í™˜"""
        try:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                reserved = torch.cuda.memory_reserved() / 1024**3   # GB
                return f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB"
            return "CUDA not available"
        except Exception as e:
            return f"GPU ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}"

    @staticmethod
    def optimize_model_loading():
        """ëª¨ë¸ ë¡œë”© ìµœì í™”"""
        try:
            if torch.cuda.is_available():
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
                print("âœ… ëª¨ë¸ ë¡œë”© ìµœì í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ìµœì í™” ì‹¤íŒ¨: {e}")

    @staticmethod
    def check_memory():
        """ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬: {memory.used/1024**3:.2f}GB / {memory.total/1024**3:.2f}GB ì‚¬ìš© ì¤‘")
            print(MemoryManager.get_gpu_memory_info())
        except ImportError:
            print("psutil íŒ¨í‚¤ì§€ê°€ ì—†ì–´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸ ë¶ˆê°€")
        except Exception as e:
            print(f"ë©”ëª¨ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
