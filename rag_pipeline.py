"""
Korean Grammar RAG System - Complete Pipeline Implementation (A100 ìµœì í™” ë²„ì „)
ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•©í•œ ë©”ì¸ ì‹œìŠ¤í…œ
"""

import json
import warnings
from typing import List, Dict, Any
try:
    from tqdm import tqdm
except ImportError:
    # tqdmì´ ì—†ìœ¼ë©´ ê¸°ë³¸ range ì‚¬ìš©
    def tqdm(iterable, *args, **kwargs):
        return iterable

from models import (
    QueryRewriter, KoreanEmbedder, RankRAGModel, 
    GuidedRankSelector, FinalAnswerGenerator
)
from utils import (
    KoreanTextProcessor, HybridRetriever, MultiStageReranker,
    EvaluationMetrics, DataLoader, MemoryManager
)

warnings.filterwarnings('ignore')

class KoreanGrammarRAGSystem:
    """
    Complete Korean Grammar RAG System (A100 ìµœì í™” ë²„ì „)
    í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” RAG ì‹œìŠ¤í…œ - ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
    """

    def __init__(self, enable_llm=True):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            enable_llm (bool): ì‹¤ì œ LLM ì‚¬ìš© ì—¬ë¶€ (Falseì‹œ í…œí”Œë¦¿ ëª¨ë“œ)
        """
        self.enable_llm = enable_llm

        # LLM ëª¨ë¸ë“¤ (ì§€ì—° ë¡œë”©)
        self.query_rewriter = QueryRewriter() if enable_llm else None
        self.embedder = KoreanEmbedder() if enable_llm else None
        self.rankrag_model = RankRAGModel() if enable_llm else None
        self.guided_selector = GuidedRankSelector() if enable_llm else None
        self.final_generator = FinalAnswerGenerator() if enable_llm else None

        # ê²€ìƒ‰ ë° ì¬ë­í‚¹ ì‹œìŠ¤í…œ
        self.hybrid_retriever = None
        self.reranker = MultiStageReranker()

        # ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_chunks = []

        # A100 ìµœì í™”ë¥¼ ìœ„í•œ ëª¨ë¸ ê´€ë¦¬
        self.models = {
            'query_rewriter': self.query_rewriter,
            'embedder': self.embedder,
            'rankrag': self.rankrag_model,
            'guided_selector': self.guided_selector,
            'final_generator': self.final_generator
        }
        self.current_model = None

        print(f"ğŸš€ Korean Grammar RAG System initialized (LLM: {enable_llm})")


    def cleanup(self):
        """
        ë¦¬ì†ŒìŠ¤ ì •ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì— ì‘ì„±. ì—†ìœ¼ë©´ íŒ¨ìŠ¤.
        """
        try:
            # ëª¨ë“  ëª¨ë¸ ì •ë¦¬
            for model_name, model in self.models.items():
                if model is not None:
                    del model
            
            # ê¸°íƒ€ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if hasattr(self, 'hybrid_retriever') and self.hybrid_retriever:
                del self.hybrid_retriever
                
            import gc
            gc.collect()
            print("ğŸ§¹ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def process_question_optimized(self, question_data):
        """ìˆœì°¨ì  ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½"""
        try:
            question = question_data.get('question', '')
            question_type = question_data.get('question_type', 'ì„ íƒí˜•')
            
            print(f"ğŸ”„ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question[:50]}...")
            
            # ê¸°ë³¸ process_question í˜¸ì¶œ
            result = self.process_question(question, question_type)
            
            # ê²°ê³¼ì—ì„œ ë‹µë³€ ì¶”ì¶œ
            final_answer = result.get('final_answer') or result.get('rankrag_answer')
            
            # ë‹µë³€ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ fallback
            if not final_answer or len(final_answer.strip()) < 5:
                print("âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨, fallback ë‹µë³€ ìƒì„±")
                final_answer = self.generate_fallback_answer(question_data)
            
            contexts_used = len(result.get('reranked_contexts', []))
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ: {final_answer[:100]}...")
            
            return {
                'predicted_answer': final_answer,
                'contexts_used': contexts_used
            }
            
        except Exception as e:  # âœ… tryì™€ ë™ì¼í•œ ë“¤ì—¬ì“°ê¸° ë ˆë²¨ (4ì¹¸)
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            
            # fallback ë‹µë³€
            return {
                'predicted_answer': self.generate_fallback_answer(question_data),
                'contexts_used': 0
            }
        finally:  # âœ… tryì™€ ë™ì¼í•œ ë“¤ì—¬ì“°ê¸° ë ˆë²¨ (4ì¹¸)
            # í•­ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬
            try:
                self.unload_current_model()
            except:
                pass
    
    def load_model_on_demand(self, model_name):
        """í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ ë¡œë“œ"""
        if not self.enable_llm or model_name not in self.models:
            return False
            
        # í˜„ì¬ ëª¨ë¸ê³¼ ë‹¤ë¥´ë©´ ì •ë¦¬í•˜ê³  ìƒˆë¡œ ë¡œë“œ
        if self.current_model != model_name:
            if self.current_model:
                self.unload_current_model()
            
            model = self.models[model_name]
            if model:
                try:
                    model.load_model()
                    if model.is_loaded:
                        self.current_model = model_name
                        print(f"âœ… {model_name} loaded successfully")
                        return True
                except Exception as e:
                    print(f"âŒ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
                    MemoryManager.clear_gpu_memory()
                    return False
        else:
            return self.models[model_name].is_loaded if self.models[model_name] else False
        
        return False
    
    def unload_current_model(self):
        """í˜„ì¬ ëª¨ë¸ ì–¸ë¡œë“œ ë©”ì„œë“œ"""
        try:
            if self.current_model:
                del self.current_model
                self.current_model = None
                
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    print("ğŸ§¹ CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except ImportError:
                pass
                
            print("ğŸ§¹ ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    def load_knowledge_base(self, train_data_path: str):
        """ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•"""
        print("ğŸ“š Loading knowledge base...")

        # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
        train_data = DataLoader.load_json_dataset(train_data_path)

        # ì§€ì‹ ì²­í¬ ìƒì„±
        self.knowledge_chunks = DataLoader.create_knowledge_chunks_from_data(train_data)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        embedder = None
        if self.enable_llm and self.embedder:
            # ë©”ëª¨ë¦¬ ì²´í¬ í›„ ì„ë² ë” ë¡œë“œ ê²°ì •
            memory_status = MemoryManager.check_memory_status()
            if memory_status.get('usage_ratio', 1.0) < 0.7:  # 70% ë¯¸ë§Œì¼ ë•Œë§Œ
                embedder = self.embedder
        
        self.hybrid_retriever = HybridRetriever(self.knowledge_chunks, embedder)

        print(f"âœ… Knowledge base loaded: {len(self.knowledge_chunks)} chunks")

        if self.enable_llm and self.embedder:
            print("ğŸ”„ Building dense embeddings...")
            # ì„ë² ë”© ë¯¸ë¦¬ ë¡œë“œ (ë©”ëª¨ë¦¬ í—ˆìš©ì‹œ)
            if self.load_model_on_demand('embedder'):
                # ë¡œë“œ í›„ ë°”ë¡œ ì–¸ë¡œë“œ (ê²€ìƒ‰ì‹œ í•„ìš”í•˜ë©´ ë‹¤ì‹œ ë¡œë“œ)
                self.unload_current_model()

    def enhance_query(self, question: str) -> List[str]:
        """ì¿¼ë¦¬ í–¥ìƒ ë° í™•ì¥"""
        enhanced_queries = []

        # 1. ê¸°ë³¸ ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = KoreanTextProcessor.normalize_korean_text(question)
        enhanced_queries.append(normalized_query)

        # 2. ì„ íƒì§€ í™•ì¥
        option_expanded = KoreanTextProcessor.expand_query_with_options(question)
        enhanced_queries.extend(option_expanded)

        # 3. LLM ê¸°ë°˜ ì¿¼ë¦¬ ì¬ì‘ì„± (HyDE) - ë©”ëª¨ë¦¬ ì•ˆì „
        if self.enable_llm and self.query_rewriter:
            try:
                if self.load_model_on_demand('query_rewriter'):
                    llm_expanded = self.query_rewriter.rewrite_query(question)
                    if llm_expanded and llm_expanded != question:
                        enhanced_queries.append(llm_expanded)
            except Exception as e:
                print(f"âš ï¸ Query rewriting failed: {e}")

        # ì¤‘ë³µ ì œê±°
        unique_queries = list(dict.fromkeys(enhanced_queries))

        return unique_queries

    def retrieve_contexts(self, queries: List[str], top_k: int = 10) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if not self.hybrid_retriever:
            return []

        all_contexts = []

        # ê° í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        for query in queries:
            contexts = self.hybrid_retriever.hybrid_search(query, top_k=top_k//len(queries) + 2)
            all_contexts.extend(contexts)

        # ì¤‘ë³µ ì œê±° (ID ê¸°ì¤€)
        seen_ids = set()
        unique_contexts = []
        for ctx in all_contexts:
            if ctx['id'] not in seen_ids:
                unique_contexts.append(ctx)
                seen_ids.add(ctx['id'])

        # ê²€ìƒ‰ ì ìˆ˜ë¡œ ì •ë ¬
        unique_contexts.sort(key=lambda x: x.get('retrieval_score', 0), reverse=True)

        return unique_contexts[:top_k]

    def rerank_contexts(self, question: str, question_type: str, contexts: List[Dict]) -> List[Dict]:
        """ë‹¤ë‹¨ê³„ ì¬ë­í‚¹"""
        if not contexts:
            return []

        reranked = self.reranker.rerank_contexts(question, question_type, contexts)
        return reranked

    def rank_contexts_with_llm(self, question: str, contexts: List[Dict]) -> str:
        """LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë­í‚¹ ì„¤ëª…"""
        if not self.enable_llm or not self.guided_selector or not contexts:
            return "ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìœ„í•œ LLMì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        try:
            if self.load_model_on_demand('guided_selector'):
                explanation = self.guided_selector.explain_context_ranking(question, contexts[:3])
                return explanation
            else:
                return "ì»¨í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨"
        except Exception as e:
            print(f"âš ï¸ LLM guided ranking failed: {e}")
            return f"ì»¨í…ìŠ¤íŠ¸ ë­í‚¹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    def generate_answer_with_rankrag(self, question: str, question_type: str, contexts: List[Dict]) -> str:
        """RankRAG ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
        if not self.enable_llm or not self.rankrag_model or not contexts:
            return self._generate_template_answer(question, question_type, contexts)

        try:
            if self.load_model_on_demand('rankrag'):
                answer = self.rankrag_model.rank_and_generate(question, contexts, question_type)
                return answer
            else:
                return self._generate_template_answer(question, question_type, contexts)
        except Exception as e:
            print(f"âš ï¸ RankRAG generation failed: {e}")
            return self._generate_template_answer(question, question_type, contexts)

    def generate_final_answer(self, question: str, question_type: str, 
                            selected_contexts: List[Dict], context_explanation: str) -> str:
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        if not self.enable_llm or not self.final_generator or not selected_contexts:
            return self._generate_template_answer(question, question_type, selected_contexts)

        try:
            if self.load_model_on_demand('final_generator'):
                answer = self.final_generator.generate_final_answer(
                    question, question_type, selected_contexts, context_explanation
                )
                return answer
            else:
                return self._generate_template_answer(question, question_type, selected_contexts)
        except Exception as e:
            print(f"âš ï¸ Final answer generation failed: {e}")
            return self._generate_template_answer(question, question_type, selected_contexts)

    def generate_answer_sequential(self, question_data, contexts):
        """ìˆœì°¨ì  ë‹µë³€ ìƒì„± (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        question = question_data.get('question', '')
        question_type = question_data.get('question_type', 'ì„ íƒí˜•')
        
        try:
            # 1ì°¨ ì‹œë„: RankRAG
            if self.load_model_on_demand('rankrag'):
                answer = self.rankrag_model.rank_and_generate(question, contexts, question_type)
                if answer and len(answer.strip()) > 10:
                    return answer
        except Exception as e:
            print(f"âŒ RankRAG ì‹¤íŒ¨: {e}")
        
        try:
            # 2ì°¨ ì‹œë„: Final Answer Generator
            if self.load_model_on_demand('final_generator'):
                answer = self.final_generator.generate_final_answer(
                    question, question_type, contexts[:3], ""
                )
                if answer and len(answer.strip()) > 10:
                    return answer
        except Exception as e:
            print(f"âŒ Final Generator ì‹¤íŒ¨: {e}")
        
        # 3ì°¨ ì‹œë„: í…œí”Œë¦¿ ê¸°ë°˜
        return self.generate_template_answer(question_data, contexts)

    def generate_template_answer(self, question_data, contexts):
        """í…œí”Œë¦¿ ê¸°ë°˜ fallback ë‹µë³€"""
        question = question_data.get('question', '')
        question_type = question_data.get('question_type', 'ì„ íƒí˜•')
        return self._generate_template_answer(question, question_type, contexts)

    def generate_fallback_answer(self, question_data):
        """í™•ì‹¤í•œ fallback ë‹µë³€ ìƒì„±"""
        question = question_data.get('question', '')
        question_type = question_data.get('question_type', 'ì„ íƒí˜•')
        
        # ì„ íƒì§€ ì¶”ì¶œ
        if '{' in question and '}' in question:
            start = question.find('{')
            end = question.find('}')
            if start != -1 and end != -1:
                options_text = question[start+1:end]
                options = [opt.strip() for opt in options_text.split('/')]
                
                if len(options) >= 2:
                    if question_type == "ì„ íƒí˜•":
                        return f'"{options[1]}"ì´ ì˜³ë‹¤. í•œêµ­ì–´ ì–´ë¬¸ ê·œë²”ì— ë”°ë¥¸ ì˜¬ë°”ë¥¸ í‘œê¸°ì…ë‹ˆë‹¤.'
                    
        if question_type == "êµì •í˜•":
            return "ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë§ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ êµì •í•´ì•¼ í•©ë‹ˆë‹¤."
        else:
            return "ì£¼ì–´ì§„ ì„ íƒì§€ ì¤‘ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì„ íƒí•˜ì—¬ì•¼ í•©ë‹ˆë‹¤."

    def _generate_template_answer(self, question: str, question_type: str, contexts: List[Dict]) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„± (LLM ì—†ì´)"""
        if not contexts:
            return f"ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸: {question}"

        # ìµœê³  ì ìˆ˜ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        best_context = contexts[0]

        # ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€
        if question_type == "ì„ íƒí˜•":
            # ì„ íƒì§€ ì¶”ì¶œ ì‹œë„
            options = KoreanTextProcessor.extract_options_from_question(question)
            if options and len(options) >= 2:
                # ì²« ë²ˆì§¸ ì˜µì…˜ì„ ì •ë‹µìœ¼ë¡œ ê°€ì • (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                answer = f'"{options[0]}"ì´ ì˜³ë‹¤. {best_context["text"][:200]}...'
            else:
                answer = f"ì£¼ì–´ì§„ ì„ íƒì§€ ì¤‘ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. {best_context['text'][:200]}..."
        else:  # êµì •í˜•
            answer = f"ì–´ë¬¸ ê·œë²”ì— ë§ê²Œ êµì •ì´ í•„ìš”í•©ë‹ˆë‹¤. {best_context['text'][:200]}..."

        return answer

    def process_question(self, question: str, question_type: str) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        results = {
            'question': question,
            'question_type': question_type,
            'enhanced_queries': [],
            'retrieved_contexts': [],
            'reranked_contexts': [],
            'context_explanation': '',
            'rankrag_answer': '',
            'final_answer': '',
            'processing_info': {}
        }

        try:
            # 1. ì¿¼ë¦¬ í–¥ìƒ
            print("ğŸ”„ Step 1: Query Enhancement")
            enhanced_queries = self.enhance_query(question)
            results['enhanced_queries'] = enhanced_queries

            # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            print("ğŸ”„ Step 2: Hybrid Retrieval")
            retrieved_contexts = self.retrieve_contexts(enhanced_queries, top_k=10)
            results['retrieved_contexts'] = retrieved_contexts

            # 3. ë‹¤ë‹¨ê³„ ì¬ë­í‚¹
            print("ğŸ”„ Step 3: Multi-stage Reranking")
            reranked_contexts = self.rerank_contexts(question, question_type, retrieved_contexts)
            results['reranked_contexts'] = reranked_contexts

            # 4. LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë­í‚¹ ì„¤ëª…
            print("ğŸ”„ Step 4: LLM Guided Ranking")
            context_explanation = self.rank_contexts_with_llm(question, reranked_contexts[:3])
            results['context_explanation'] = context_explanation

            # 5. RankRAG ë‹µë³€ ìƒì„±
            print("ğŸ”„ Step 5: RankRAG Answer Generation")
            rankrag_answer = self.generate_answer_with_rankrag(
                question, question_type, reranked_contexts[:5]
                        )
            results['rankrag_answer'] = rankrag_answer

            # 6. ìµœì¢… ë‹µë³€ ìƒì„±
            print("ğŸ”„ Step 6: Final Answer Generation")
            final_answer = self.generate_final_answer(
                question, question_type, reranked_contexts[:3], context_explanation
            )
            results['final_answer'] = final_answer

            # ì²˜ë¦¬ ì •ë³´
            results['processing_info'] = {
                'num_enhanced_queries': len(enhanced_queries),
                'num_retrieved_contexts': len(retrieved_contexts),
                'num_reranked_contexts': len(reranked_contexts),
                'top_context_score': reranked_contexts[0]['final_score'] if reranked_contexts else 0,
                'memory_info': MemoryManager.get_gpu_memory_info()
            }

            print("âœ… Question processed successfully")

        except Exception as e:
            print(f"âŒ Error processing question: {e}")
            results['error'] = str(e)

        finally:
            # í•­ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬
            self.unload_current_model()

        return results



    def enhance_query_if_needed(self, question: str) -> List[str]:
        """í•„ìš”ì‹œì—ë§Œ ì¿¼ë¦¬ í–¥ìƒ"""
        enhanced_queries = []
        
        # ê¸°ë³¸ ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = KoreanTextProcessor.normalize_korean_text(question)
        enhanced_queries.append(normalized_query)

        # ì„ íƒì§€ í™•ì¥
        option_expanded = KoreanTextProcessor.expand_query_with_options(question)
        enhanced_queries.extend(option_expanded)

        # ë©”ëª¨ë¦¬ ì—¬ìœ ê°€ ìˆì„ ë•Œë§Œ LLM ì¿¼ë¦¬ ì¬ì‘ì„±
        memory_status = MemoryManager.check_memory_status()
        if memory_status.get('usage_ratio', 1.0) < 0.6:  # 60% ë¯¸ë§Œì¼ ë•Œë§Œ
            if self.enable_llm and self.query_rewriter:
                try:
                    if self.load_model_on_demand('query_rewriter'):
                        llm_expanded = self.query_rewriter.rewrite_query(question)
                        if llm_expanded and llm_expanded != question:
                            enhanced_queries.append(llm_expanded)
                except Exception as e:
                    print(f"âš ï¸ Query rewriting skipped: {e}")

        return list(dict.fromkeys(enhanced_queries))

    def evaluate_on_dataset(self, test_data_path: str, output_path: str = None, 
                           max_samples: int = None) -> Dict[str, float]:
        """ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ ìˆ˜í–‰"""
        print(f"ğŸ“Š Starting evaluation on dataset: {test_data_path}")

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_data = DataLoader.load_json_dataset(test_data_path)

        if max_samples:
            test_data = test_data[:max_samples]

        results = []
        correct_predictions = 0
        total_predictions = len(test_data)

        for i, sample in enumerate(tqdm(test_data, desc="Processing samples")):
            question = sample['input']['question']
            question_type = sample['input']['question_type']
            ground_truth = sample.get('output', {}).get('answer', '')

            # ë©”ëª¨ë¦¬ ì²´í¬ ë° ìë™ ì •ë¦¬
            MemoryManager.auto_cleanup_if_needed()

            # ì§ˆë¬¸ ì²˜ë¦¬
            result = self.process_question(question, question_type)

            # ìµœì¢… ë‹µë³€ ì„ íƒ (RankRAG ë˜ëŠ” Final Answer)
            predicted_answer = result['final_answer'] or result['rankrag_answer']

            # í‰ê°€ (ground truthê°€ ìˆëŠ” ê²½ìš°)
            is_correct = False
            if ground_truth:
                is_correct = EvaluationMetrics.exact_match(predicted_answer, ground_truth)
                if is_correct:
                    correct_predictions += 1

            # ê²°ê³¼ ì €ì¥
            sample_result = {
                'id': sample.get('id', i),
                'input': sample['input'],
                'predicted_answer': predicted_answer,
                'ground_truth': ground_truth,
                'is_correct': is_correct,
                'processing_details': result
            }

            results.append(sample_result)

            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
            if i % 5 == 0:  # 5ê°œë§ˆë‹¤ ì •ë¦¬
                MemoryManager.clear_gpu_memory()
                # ì¤‘ê°„ ì €ì¥
                if output_path:
                    DataLoader.save_intermediate_results(results, output_path, i+1)

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        accuracy = correct_predictions / total_predictions if ground_truth else 0.0

        evaluation_metrics = {
            'accuracy': accuracy,
            'total_samples': total_predictions,
            'correct_predictions': correct_predictions,
            'average_contexts_used': sum(
                r['processing_details']['processing_info']['num_reranked_contexts'] 
                for r in results
            ) / len(results)
        }

        print(f"ğŸ“Š Evaluation Results:")
        print(f"   Accuracy: {accuracy:.4f}")
        print(f"   Total Samples: {total_predictions}")
        print(f"   Correct Predictions: {correct_predictions}")

        # ê²°ê³¼ ì €ì¥
        if output_path:
            final_output = {
                'evaluation_metrics': evaluation_metrics,
                'predictions': results
            }
            DataLoader.save_results(final_output, output_path)
            print(f"ğŸ’¾ Results saved to: {output_path}")

        return evaluation_metrics

    def cleanup(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬"""
        print("ğŸ§¹ Cleaning up system resources...")
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì–¸ë¡œë“œ
        self.unload_current_model()
        
        # ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.clear_gpu_memory(force=True)

        # ëª¨ë¸ë“¤ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        for model_name, model in self.models.items():
            if model and hasattr(model, 'model'):
                try:
                    del model.model
                    if hasattr(model, 'tokenizer'):
                        del model.tokenizer
                    model.is_loaded = False
                except:
                    pass

        print("âœ… Cleanup completed")

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_rag_system(enable_llm: bool = True) -> KoreanGrammarRAGSystem:
    """RAG ì‹œìŠ¤í…œ ìƒì„±"""
    return KoreanGrammarRAGSystem(enable_llm=enable_llm)

def quick_test(system: KoreanGrammarRAGSystem, question: str, question_type: str = "ì„ íƒí˜•"):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ§ª Testing question: {question}")
    result = system.process_question(question, question_type)
    print(f"ğŸ“ Answer: {result['final_answer'] or result['rankrag_answer']}")
    return result

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ìƒì„± ë° í…ŒìŠ¤íŠ¸
    rag_system = create_rag_system(enable_llm=True)

    # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    rag_system.load_knowledge_base('.//korean_language_rag_V1.0_train.json')

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    test_question = "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”."

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = quick_test(rag_system, test_question, "ì„ íƒí˜•")

    # ì •ë¦¬
    rag_system.cleanup()
