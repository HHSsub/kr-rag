#!/usr/bin/env python3
"""
Korean Grammar RAG System - Lightweight Demo
PyTorch ì—†ì´ë„ ì‘ë™í•˜ëŠ” ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import time
import re
from pathlib import Path

class LightweightDemo:
    """PyTorch ì˜ì¡´ì„± ì—†ëŠ” ê²½ëŸ‰ ë°ëª¨"""

    def __init__(self):
        self.knowledge_chunks = []

    def load_knowledge_base(self, train_path):
        """ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ"""
        if not Path(train_path).exists():
            print(f"âŒ Training data not found: {train_path}")
            return False

        with open(train_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)

        # ì§€ì‹ ì²­í¬ ìƒì„±
        for i, item in enumerate(train_data):
            question = item['input']['question']
            answer = item['output']['answer']
            question_type = item['input']['question_type']

            knowledge_text = f"{question} {answer}"

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category = "ê¸°íƒ€"
            if any(word in knowledge_text for word in ['ë§ì¶¤ë²•', 'ì² ì', 'ì–´ê°„', 'ì–´ë¯¸']):
                category = "ë§ì¶¤ë²•"
            elif any(word in knowledge_text for word in ['ë„ì–´ì“°ê¸°', 'ë„ì–´', 'ë¶™ì—¬']):
                category = "ë„ì–´ì“°ê¸°"
            elif any(word in knowledge_text for word in ['í‘œì¤€ì–´', 'í‘œì¤€', 'ì‚¬ì •']):
                category = "í‘œì¤€ì–´"
            elif any(word in knowledge_text for word in ['ë¬¸ì¥ë¶€í˜¸', 'ë§ˆì¹¨í‘œ', 'ì‰¼í‘œ']):
                category = "ë¬¸ì¥ë¶€í˜¸"
            elif any(word in knowledge_text for word in ['ì™¸ë˜ì–´', 'í‘œê¸°ë²•']):
                category = "ì™¸ë˜ì–´í‘œê¸°"

            chunk = {
                'id': f"chunk_{i}",
                'text': knowledge_text,
                'category': category,
                'question_type': question_type,
                'source': 'training_data'
            }

            self.knowledge_chunks.append(chunk)

        print(f"âœ… Knowledge base loaded: {len(self.knowledge_chunks)} chunks")
        return True

    def extract_options_from_question(self, question):
        """ì§ˆë¬¸ì—ì„œ ì„ íƒì§€ ì¶”ì¶œ"""
        pattern = r'{([^}]+)}'
        matches = re.findall(pattern, question)

        options = []
        for match in matches:
            if '/' in match:
                options.extend([opt.strip() for opt in match.split('/')])

        return options

    def simple_search(self, question, top_k=5):
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        question_words = set(question.split())

        scored_chunks = []
        for chunk in self.knowledge_chunks:
            chunk_words = set(chunk['text'].split())

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            common_words = question_words & chunk_words
            score = len(common_words) / len(question_words) if question_words else 0

            # ì„ íƒì§€ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            options = self.extract_options_from_question(question)
            for option in options:
                if option in chunk['text']:
                    score += 0.3

            scored_chunks.append((chunk, score))

        # ì ìˆ˜ë¡œ ì •ë ¬
        scored_chunks.sort(key=lambda x: x[1], reverse=True)

        return [chunk for chunk, score in scored_chunks[:top_k]]

    def generate_template_answer(self, question, question_type, contexts):
        """í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        if not contexts:
            return f"ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸: {question}"

        best_context = contexts[0]

        if question_type == "ì„ íƒí˜•":
            options = self.extract_options_from_question(question)
            if options and len(options) >= 2:
                # ì²« ë²ˆì§¸ ì˜µì…˜ì„ ì •ë‹µìœ¼ë¡œ ê°€ì • (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
                answer = f'"{options[0]}"ì´ ì˜³ë‹¤. {best_context["text"][len(question):300]}...'
            else:
                answer = f"ì£¼ì–´ì§„ ì„ íƒì§€ ì¤‘ ì˜¬ë°”ë¥¸ í‘œí˜„ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. {best_context['text'][:300]}..."
        else:  # êµì •í˜•
            answer = f"ì–´ë¬¸ ê·œë²”ì— ë§ê²Œ êµì •ì´ í•„ìš”í•©ë‹ˆë‹¤. {best_context['text'][:300]}..."

        return answer

    def process_question(self, question, question_type):
        """ì§ˆë¬¸ ì²˜ë¦¬"""
        print(f"ğŸ”„ Processing: {question_type} question")

        # 1. ê²€ìƒ‰
        contexts = self.simple_search(question)
        print(f"ğŸ” Found {len(contexts)} relevant contexts")

        # 2. ë‹µë³€ ìƒì„±
        answer = self.generate_template_answer(question, question_type, contexts)

        return {
            'question': question,
            'question_type': question_type,
            'contexts_used': len(contexts),
            'answer': answer
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ‡°ğŸ‡· Korean Grammar RAG System - Lightweight Demo")
    print("=" * 60)
    print("âš ï¸  This is a lightweight demo that works without PyTorch")
    print("    For full LLM functionality, install dependencies and use main.py")
    print()

    # ì‹œìŠ¤í…œ ìƒì„±
    demo = LightweightDemo()

    # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ
    train_path = './/korean_language_rag_V1.0_train.json'
    if not demo.load_knowledge_base(train_path):
        print("âŒ Could not load knowledge base. Exiting.")
        return

    # ë°ëª¨ ì§ˆë¬¸ë“¤
    demo_questions = [
        {
            "question": "ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
            "type": "ì„ íƒí˜•"
        },
        {
            "question": "ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n\"ì™¸ì¶œì‹œì—ëŠ” ì—ì–´ì»¨ì„ ê¼­ ë•ì‹œë‹¤.\"",
            "type": "êµì •í˜•"
        },
        {
            "question": "{ê²€/ê»Œ}ì„ ì”¹ë‹¤. ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
            "type": "ì„ íƒí˜•"
        }
    ]

    # ê° ì§ˆë¬¸ ì²˜ë¦¬
    total_time = 0
    for i, demo_q in enumerate(demo_questions, 1):
        print(f"\nğŸ“ Demo Question {i}: {demo_q['type']}")
        print(f"Q: {demo_q['question']}")
        print("-" * 50)

        start_time = time.time()
        result = demo.process_question(demo_q['question'], demo_q['type'])
        processing_time = time.time() - start_time
        total_time += processing_time

        print(f"A: {result['answer']}")
        print(f"â±ï¸  Processing time: {processing_time:.3f}s")
        print(f"ğŸ“Š Contexts used: {result['contexts_used']}")

    print(f"\nâœ… Demo completed successfully!")
    print(f"ğŸ“Š Total processing time: {total_time:.3f}s")
    print(f"âš¡ Average time per question: {total_time/len(demo_questions):.3f}s")

    print("\nğŸš€ To run with full LLM functionality:")
    print("   1. Install dependencies: pip install -r requirements.txt")
    print("   2. Run: python main.py --mode demo --enable_llm")

if __name__ == "__main__":
    main()
